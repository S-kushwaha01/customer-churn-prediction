# Customer Churn Prediction using Machine Learning

## ğŸ“ŒProject Overview
Customer churn refers to customers who stop using a companyâ€™s service.  
In this project, we build a machine learning model to predict customer churn using historical customer data from a telecom company.

The goal is to identify customers who are likely to leave, so businesses can take preventive actions such as discounts, better support, or personalized offers.

## ğŸ¯ Problem Statement
Telecom companies lose revenue when customers churn.  
This project aims to:  
- Analyze customer behavior
- Identify factors contributing to churn
- Build ML models to predict churn accurately

 
## ğŸ“‚ Dataset Description
Dataset: IBM Telco Customer Churn Dataset   
- Total Records: 7,043 customers  
- Total Features: 21  
- Target Variable: Churn (Yes = customer left, No = customer stayed)  

Key Features:  
- `tenure` â€“ Number of months customer stayed  
- `MonthlyCharges` â€“ Monthly bill amount  
- `TotalCharges` â€“ Total amount charged  
- `Contract` â€“ Contract type  
- `PaymentMethod` â€“ Payment method  
- `InternetService` â€“ Internet service type

## ğŸ” Exploratory Data Analysis (EDA) â€“ Key Insights
- Dataset is **imbalanced**, reflecting real-world customer behavior
- Customers with low `tenure` are more likely to churn
- Higher `MonthlyCharges` increase churn risk
- `Month-to-month` contracts have the highest churn rate
- Customers using `Electronic check` payment method churn more
- `Long-term contracts` significantly reduce churn


## âš™ï¸ Data Preprocessing & Feature Engineering
- Converted `TotalCharges` to numeric format
- Handled missing values using **median imputation**
- Encoded categorical variables using **One-Hot Encoding**
- Split data into training and testing sets (**80/20**)
- Applied **feature scaling** where required

---

## âš–ï¸ Class Imbalance Handling
Instead of using a single global resampling method, class imbalance was handled **individually for each model** using model-specific techniques, such as:
- `Class weight` adjustment
- `Decision threshold` tuning
- `Model-internal` imbalance handling

This approach resulted in **better and more stable performance** compared to applying **SMOTE globally**.


## ğŸ¤– Machine Learning Models Used
- `Logistic Regression` â€“ Baseline model
- `Support Vector Machine (SVM)`
- `XGBoost Classifier`

---

## ğŸ“Š Evaluation Metrics
- `Accuracy`
- `Precision`
- `Recall` *(primary metric)*
- `F1 Score`


